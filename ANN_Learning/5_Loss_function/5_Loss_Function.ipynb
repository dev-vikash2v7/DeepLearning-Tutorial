{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What Are Loss Functions?\n",
    "In neural networks, loss functions help optimize the performance of the model. They are usually used to measure some penalty that the model incurs on its predictions, such as the deviation of the prediction away from the ground truth label. Loss functions are usually differentiable across their domain (but it is allowed that the gradient is undefined only for very specific points, such as x = 0, which is basically ignored in practice). In the training loop, they are differentiated with respect to parameters, and these gradients are used for your backpropagation and gradient descent steps to optimize your model on the training set.\n",
    "\n",
    "Loss functions are also slightly different from metrics. While loss functions can tell you the performance of our model, they might not be of direct interest or easily explainable by humans. This is where metrics come in. Metrics such as accuracy are much more useful for humans to understand the performance of a neural network even though they might not be good choices for loss functions since they might not be differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types \n",
    "\n",
    "1. Probabilistic losses\n",
    "\n",
    "- BinaryCrossentropy class (Log Loss)\n",
    "- CategoricalCrossentropy class\n",
    "- SparseCategoricalCrossentropy class\n",
    "- Poisson class\n",
    "- KLDivergence class\n",
    "- kl_divergence function \n",
    "\n",
    "2.  Regression losses\n",
    "\n",
    "- MeanSquaredError class\n",
    "- MeanAbsoluteError class\n",
    "- MeanAbsolutePercentageError class\n",
    "- MeanSquaredLogarithmicError class\n",
    "- CosineSimilarity class\n",
    "- Huber class\n",
    "- LogCosh class\n",
    "\n",
    "3. Hinge losses for \"maximum-margin\" classification\n",
    "\n",
    "-  Hinge class\n",
    "-  SquaredHinge class\n",
    "-  CategoricalHinge class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Labels\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Means Squared Error (MSE):\n",
    "\n",
    "MSE tells, how close a regression line from predicted points. And this is done simply by taking distance from point to the regression line and squaring them. The squaring is a must so it’ll remove the negative sign problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_true = [[10., 10.],\n",
    "          [0., 0.]]\n",
    "# Predicted Labels\n",
    "y_pred = [[10., 10.], \n",
    "          [1., 0.]]\n",
    "#Mean Sqaured Error Loss\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "mse(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement with scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.25, array([0.5, 0. ]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mae(y_predicted, y_true):\n",
    "    total_error = 0\n",
    "    for yp, yt in zip(y_predicted, y_true):\n",
    "        total_error += abs(yp - yt)\n",
    "    mae = total_error/len(y_predicted)\n",
    "    return mae \n",
    "\n",
    "def mae_np(y_predicted, y_true):\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true)\n",
    "\n",
    "    return np.mean(np.abs(y_predicted-y_true))\n",
    "\n",
    "\n",
    "mae_np(y_pred, y_true) , mae(y_pred , y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Mean Absolute Error:\n",
    "\n",
    "MAE simply calculated by taking distance from point to the regression line. The MAE is more sensitive to outliers. So before using MAE confirm that data doesn’t contain outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Input Labels\n",
    "y_true = [[10., 20.],\n",
    "          [30., 40.]]\n",
    "# Predicted Labels\n",
    "y_pred = [[10., 20.], \n",
    "          [30., 0.]]\n",
    "\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "mae(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Cosine Similarity Loss:\n",
    "\n",
    "Cosine similarity is a measure of similarity between two non-zero vectors. This loss function calculates the cosine similarity between labels and predictions.\n",
    "\n",
    "It’s just a number between 1 and -1\n",
    "when it’s a negative number between -1 and 0 then, 0 indicates orthogonality, and values closer to -1 show greater similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Input Labels\n",
    "y_true = [[10., 20.],\n",
    "          [30., 40.]]\n",
    "# Predicted Labels\n",
    "y_pred = [[10., 20.], \n",
    "          [30., 40.]]\n",
    "\n",
    "cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)\n",
    "cosine_loss(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Huber Loss:\n",
    "\n",
    "The Huber loss function is quadratic for small values and linear for larger values,\n",
    "\n",
    "* For each value of X the error = y_true-y_pred\n",
    "\n",
    "* Loss = 0.5 * X^2                                   if |X| <= d\n",
    "* Loss = 0.5 * d^2 + d (|X| - d)               if |X| > d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.75"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input Labels\n",
    "y_true = [[10., 20.],\n",
    "          [30., 40.]]\n",
    "# Predicted Labels\n",
    "y_pred = [[-10., 20.], \n",
    "          [30., 0.]]\n",
    "\n",
    "hub_loss = tf.keras.losses.Huber()\n",
    "hub_loss(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. LogCosh Loss:\n",
    "\n",
    "The LogCosh loss computes the log of the hyperbolic cosine of the prediction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1084452"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_true = [[0., 1.], [0., 1.]] \n",
    "y_pred = [[0., 1.], [1., 1.]] \n",
    "logcosh_loss = tf.keras.losses.LogCosh() \n",
    "logcosh_loss(y_true, y_pred).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Loss Functions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Binary Cross-Entropy Loss:\n",
    "\n",
    "Binary cross-entropy is used to compute the cross-entropy between the true labels and predicted outputs. It’s used when two-class problems arise like cat and dog classification [1 or 0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7206007"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0.,1.], [0.,0.]] \n",
    "y_pred= [[0.5,0.4], [0.6,0.3]] \n",
    "binary_cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "binary_cross_entropy(y_true=y_true,y_pred=y_pred).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7206008970617469"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# custom building\n",
    "# loss = -mean( yt * log(yp) + (1-yt)*log(1-yp)  )\n",
    "# as log(0) = -inf \n",
    "# log(1-1) = -inf \n",
    "# therefor we convert the 0 and 1 to its nearest number btw 0 & 1\n",
    "\n",
    "def log_loss(y_true, y_predicted):\n",
    "    epsilon = 1e-15\n",
    "    y_predicted_new = [max(i,epsilon) if i==0 else min(i,1-epsilon) for i in y_predicted] # 0 => 0.00000000000000001\n",
    "    # y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new] # 1 => 0.99999999999999999\n",
    "    \n",
    "    y_predicted_new = np.array(y_predicted_new)\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))\n",
    "\n",
    "y_true = [0,1,0,0]\n",
    "y_pred = [0.5,0.4,0.6,0.3]\n",
    "\n",
    "log_loss(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Categorical Crossentropy Loss:\n",
    "\n",
    "The Categorical crossentropy loss function is used to compute loss between true labels and predicted labels.\n",
    "It’s mainly used for multiclass classifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1438693"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0, 1, 0],\n",
    "          [0, 0, 1]]\n",
    "y_pred = [[0.05, 0.95, 0.56], \n",
    "          [0.1, 0.4, 0.1]]\n",
    "\n",
    "categorical_cross_entropy = tf.keras.losses.CategoricalCrossentropy()\n",
    "categorical_cross_entropy(y_true=y_true,y_pred=y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Sparse Categorical Crossentropy Loss:\n",
    "\n",
    "It is used when there are two or more classes present in our classification task. similarly to categorical crossentropy. But there is one minor difference, between categorical crossentropy and sparse categorical crossentropy that’s in sparse categorical cross-entropy labels are expected to be provided in integers.\n",
    "\n",
    "Rather than using Sparse Categorical crossentropy we can use one-hot-encoding and convert the above\n",
    "problem into categorical crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05129344, 2.3025851 ], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [1, 2]\n",
    "#Predicted Lables\n",
    "y_pred = [[0.05, 0.95, 0],\n",
    "          [0.1, 0.8, 0.1]]\n",
    "#Implementation of Sparse Categorical Crossentropy\n",
    "tf.keras.losses.sparse_categorical_crossentropy(y_true,y_pred).numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  Poisson loss:\n",
    "\n",
    "The poison loss is the mean of elements of tensor. we can calculate poison loss like \n",
    "* y_pred – y_true*log(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input Labels\n",
    "y_true = [[0., 1.],\n",
    "          [0., 0.]]\n",
    "#Predicted Lables\n",
    "y_pred = [[1., 1.],\n",
    "          [1., 0.]]\n",
    "\n",
    "# Using 'auto'/'sum_over_batch_size.\n",
    "p = tf.keras.losses.Poisson()\n",
    "p(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Kullback-Leibler Divergence Loss:\n",
    "\n",
    "Also, called KL divergence, it’s calculated by doing a negative sum of probability of each event P and then multiplying it by the log of the probability of an event.\n",
    "\n",
    "KL(P || Q) = – sum x in X P(x) * log(Q(x) / P(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11156943"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#input Labels \n",
    "y_true = [[0, 1], [0, 0]] \n",
    "# Predicted Lables\n",
    "y_pred = [[0.7, 0.8], [0.4, 0.8]] #KL divergen loss\n",
    "kl = tf.keras.losses.KLDivergence() \n",
    "kl(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hinge Losses for ‘Maximum – Margin’ Classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Hinge Loss\n",
    "It’s mainly used for problems like maximum-margin most notably for support vector machines.\n",
    "In Hinge loss values are expected to be -1 or 1. In the case of binary i.e. 0 or 1 it’ll get converted into -1 and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.25"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0., 1.], [0., 0.]] \n",
    "y_pred = [[0.5, 0.4], [0.4, 0.5]] \n",
    "h =tf.keras.losses.Hinge() \n",
    "h(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Squared Hinge Loss:\n",
    "* The Square Hinge loss is just square of hinge loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.705"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0., 1.], [0., 0.]] \n",
    "y_pred = [[0.5, 0.4], [0.4, 0.5]] \n",
    "h =tf.keras.losses.SquaredHinge() \n",
    "h(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Categorical Hinge Loss:\n",
    "* It calculates the categorical hing loss between y_true and y_pred labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [[0., 1.], [0., 0.]] \n",
    "y_pred = [[0.5, 0.4], [0.4, 0.5]] \n",
    "h =tf.keras.losses.CategoricalHinge() \n",
    "h(y_true, y_pred).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
